<html>

<head>


    <meta charset="utf-8" />
    <title>Vysics</title>

    <meta content="SceneComplete is an open-world 3D scene completion system, that constructs a complete, segmented, 3D model of a scene from a single RGB-D image."
        name="description" />
    <meta content="SceneComplete is an open-world 3D scene completion system." property="og:title" />
    <meta content="SceneComplete is an open-world 3D scene completion system, that constructs a complete, segmented, 3D model of a scene from a single RGB-D image."
        name="description"
        property="og:description" />
    <meta content="https://scenecomplete.github.io/static/images/meta.png" property="og:image" />
    <meta content="SceneComplete is an open-world 3D scene completion system." property="twitter:title" />
    <meta content="SceneComplete is an open-world 3D scene completion system, that constructs a complete, segmented, 3D model of a scene from a single RGB-D image."
        name="description"
        property="twitter:description" />
    <meta content="https://scenecomplete.github.io/static/images/meta.png" property="twitter:image" />
    <meta property="og:type" content="website" />
    <meta content="summary_large_image" name="twitter:card" />
    <meta name="viewport" content="width=device-width, initial-scale=1, minimum-scale=1" />


    <link href="https://fonts.googleapis.com" rel="preconnect" />
    <link href="https://fonts.gstatic.com" rel="preconnect" crossorigin="anonymous" />
    <script src="https://ajax.googleapis.com/ajax/libs/webfont/1.6.26/webfont.js" type="text/javascript"></script>
    <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro" rel="stylesheet">
    <script
        type="text/javascript">WebFont.load({ google: { families: ["Lato:100,100italic,300,300italic,400,400italic,700,700italic,900,900italic", "Montserrat:100,100italic,200,200italic,300,300italic,400,400italic,500,500italic,600,600italic,700,700italic,800,800italic,900,900italic", "Ubuntu:300,300italic,400,400italic,500,500italic,700,700italic", "Open Sans:300,300italic,400,400italic,600,600italic,700,700italic,800,800italic", "Changa One:400,400italic", "Varela Round:400", "Bungee Shade:regular", "Roboto:300,regular,500", "Bungee Outline:regular"] } });</script>
    <!--[if lt IE 9]><script src="https://cdnjs.cloudflare.com/ajax/libs/html5shiv/3.7.3/html5shiv.min.js" type="text/javascript"></script><![endif]-->

    <script src="https://ajax.googleapis.com/ajax/libs/jquery/1/jquery.min.js"></script>
    <script src="script.js" type="text/javascript"></script>

    <link href="style.css" rel="stylesheet" type="text/css" />

    <link href="./static/images/favicon.svg" rel="shortcut icon" type="image/x-icon" />

    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/4.7.0/css/font-awesome.min.css">
    <!-- <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/bulma@0.9.4/css/bulma.min.css"> -->

<!--     <script>
        // Check if the user is on a mobile device or the window width is 800px or less.
        if (window.innerWidth <= 800 || /Android|webOS|iPhone|iPad|iPod|BlackBerry|IEMobile|Opera Mini/i.test(navigator.userAgent)) {
            window.location.href = "mobile.html";  // Redirect to mobile version
        }
    </script> -->
    <script type="module" src="https://ajax.googleapis.com/ajax/libs/model-viewer/3.4.0/model-viewer.min.js"></script>
</head>

<body>
    <div class="section">
        <div class="container">
            <br>
            <h1 class="title">Vysics</h1>
            <h1 class="subheader">Object Reconstruction Under Occlusion by
                Fusing Vision and Contact-Rich Physics</h1>

            <div class="publication-authors" style="font-size:1.25rem;">
                <span class="author-block">
                    <a href="https://scholar.google.com/citations?user=lVj0WaQAAAAJ&hl=en" target="_blank">Bibit Bianchini
                    </a><sup>*1</sup>,</span>
                <span class="author-block">
                    <span class="author-block">
                        <a href="https://scholar.google.com/citations?user=70CbUXwAAAAJ&hl=en" target="_blank">Minghan Zhu
                        </a><sup>*1</sup>,</span>
                    <span class="author-block">
                        <a href="https://scholar.google.com/citations?user=hXb_RHoAAAAJ&hl=en" target="_blank">Mengti Sun</a><sup>2</sup>,
                    </span>
                    <span class="author-block">
                        <a href="https://scholar.google.com/citations?user=_6AHV9QAAAAJ&hl=en" target="_blank">Bowen Jiang</a><sup>1</sup>,
                    </span>
                    <span class="author-block">
                        <a href="https://www.cis.upenn.edu/~cjtaylor/" target="_blank">Camillo J. Taylor
                        </a><sup>1</sup>
                    </span>
                    <span class="author-block">
                        <a href="https://www.grasp.upenn.edu/people/michael-posa/" target="_blank">Michael Posa
                        </a><sup>1</sup>
                    </span>
            </div>
            <br>
            <div class="publication-authors" style="font-size:1.25rem;">
                <span class="author-block" style="display:block"><sup>*</sup>The first two authors contributed equally to this work.</span>

                <span class="author-block" style="display:block"><sup>1</sup>General Robotics, Automation, Sensing and Perception (GRASP) Laboratory, University of Pennsylvania </span>

                <span class="author-block" style="display:block"><sup>2</sup>Amazon Robotics</span>
            </div>
            <br>

            <div class="link-labels base-row">
                <!-- TODO: Update arxiv link -->
                <div class="base-col icon-col">
                    <!-- <a href="https://arxiv.org/abs/2410.23643" target="_blank"
                        class="link-block"> -->
                    <img src="https://uploads-ssl.webflow.com/51e0d73d83d06baa7a00000f/5cab99df4998decfbf9e218e_paper-01.png"
                        alt="paper"
                        sizes="(max-width: 479px) 12vw, (max-width: 767px) 7vw, (max-width: 991px) 41.8515625px, 56.6953125px"
                        srcset="https://uploads-ssl.webflow.com/51e0d73d83d06baa7a00000f/5cab99df4998decfbf9e218e_paper-01-p-500.png 500w, https://uploads-ssl.webflow.com/51e0d73d83d06baa7a00000f/5cab99df4998decfbf9e218e_paper-01.png 672w"
                        class="icon-img" />
                    </a>
                </div>
                <!-- TODO: Update code link -->
                <div class="base-col icon-col">
                    <!-- <a href="https://github.com/ebianchi/bundlenets/tree/structural-changes"
                        target="_blank" > -->
                    <img src="https://uploads-ssl.webflow.com/51e0d73d83d06baa7a00000f/5cae3b53b42ebb3dd4175a82_68747470733a2f2f7777772e69636f6e66696e6465722e636f6d2f646174612f69636f6e732f6f637469636f6e732f313032342f6d61726b2d6769746875622d3235362e706e67.png"
                        alt="code" class="icon-img github-img-icon" />
                    </a>
                </div>
                <!-- TODO: Update dataset link -->
                <div class="base-col icon-col">
                    <!-- <a href="https://github.com/ebianchi/bundlenets/tree/structural-changes"
                        target="_blank" > -->
                    <img src="./static/logos/dataset_icon.png"
                        alt="code" class="icon-img github-img-icon" />
                    </a>
                </div>
                <!-- TODO: Update Youtube link -->
                <div class="column-2 base-col icon-col">
                    <!-- <a href="https://youtu.be/Tuzhn4HWiL0" target="_blank"
                        class="link-block"> -->
                    <img src="./static/logos/youtube_1.svg"
                        alt="video" class="icon-img data-img-icon" />
                    </a>
                </div>
            </div>
            <div class="link-labels base-row">
                <div class="base-col icon-col">
                    <strong class="link-labels-text">Paper<br>(coming soon)</strong>
                </div>
                <div class="base-col icon-col">
                    <strong class="link-labels-text">Code<br>(coming soon)</strong>
                </div>
                <div class="base-col icon-col">
                    <strong class="link-labels-text">Dataset<br>(coming soon)</strong>
                </div>
                <div class="base-col icon-col">
                    <strong class="link-labels-text">Video<br>(coming soon)</strong>
                </div>
            </div>

            <br>

            <div class="base-row add-top-padding">
                <video id="main-video" autobuffer muted autoplay loop>
                    <source id="mp4"
                        src="./static/images/vysics_intro_small.mp4"
                        type="video/mp4">
                </video>
            </div>

            <h1 class="tldr">
                <b>TL;DR</b>:
                <u>Vysics</u> is a vision-and-physics framework for a robot to build an expressive geometry and dynamics
                model of a single rigid body, using a seconds-long RGBD video and the robot’s proprioception.
            </h1>

            <br>
            <!-- <div class="base-row add-top-padding">
                <h1 id="abstract">Experiments</h1>
            </div>
            <nav>
                <div class="carousel-slider-wrapper">
                    <div class = "carousel-slider">

                        <div class="video-container">
                            <video  loop playsinline muted autoplay>
                                <source autoplay src="./static/images/carousel/compressed_carousel2.mp4" type="video/mp4">
                            </video>
                        </div>

                        <div class="video-container">
                            <video  loop playsinline muted autoplay>
                                <source autoplay src="./static/images/carousel/compressed_carousel3.mp4" type="video/mp4">
                            </video>
                        </div>

                        <div class="video-container">
                            <video  loop playsinline muted autoplay>
                                <source autoplay src="./static/images/carousel/compressed_carousel4.mp4" type="video/mp4">
                            </video>
                        </div>

                        <div class="video-container">
                            <video  loop playsinline muted autoplay>
                                <source autoplay src="./static/images/carousel/compressed_carousel1.mp4" type="video/mp4">
                            </video>
                        </div>
                    </div>
                    <button class="carousel-button left" type="button">&lt;</button>
                    <button class="carousel-button right" type="button">&gt;</button>
                </div>
            </nav> -->
            <div class="base-row add-top-padding">
                <h1 id="abstract">Abstract</h1>
                <p class="paragraph">
                    We introduce <b>Vysics</b>, a vision-and-physics framework for a robot to build an expressive geometry and dynamics
                    model of a single rigid body, using a seconds-long RGBD video
                    and the robot's proprioception. While the computer vision community has built powerful visual 3D perception algorithms, cluttered environments with heavy occlusions can limit the visibility
                    of objects of interest. However, observed motion of partially occluded objects can imply physical interactions took place, such as
                    contact with a robot or the environment. These inferred contacts
                    can supplement the visible geometry with “physible geometry,”
                    which best explains the observed object motion through physics.
                    Vysics uses a vision-based tracking and reconstruction method,
                    BundleSDF, to estimate the trajectory and the visible geometry
                    from an RGBD video, and an odometry-based model learning
                    method, Physics Learning Library (PLL), to infer the “physible”
                    geometry from the trajectory through implicit contact dynamics
                    optimization. The visible and “physible” geometries jointly factor
                    into optimizing a signed distance function (SDF) to represent the
                    object shape. Vysics does not require pretraining, nor tactile
                    or force sensors. Compared with vision-only methods, Vysics
                    yields object models with higher geometric accuracy and better
                    dynamics prediction in experiments where the object interacts
                    with the robot and the environment under heavy occlusion.
                </p>
            </div>

            <!-- <div>
                <img id="splash-fig" src="data/splash.jpg" />
            </div> -->
            <br>
            <div class="base-row add-top-padding">
                <h1 id="abstract">Architecture</h1>
                <video id="arch-video" autoplay muted loop playsinline height="100%">
                    <source src="./static/images/vysics_architecture_animation.mp4" type="video/mp4">
                </video>
                <p class="paragraph">
                    The figure illustrates the overall design of <b>Vysics</b> from input RGBD videos and
                    robot states (left) to URDF output (right). Its core components are
                    <!-- TODO:  link to BundleSDF? -->
                    <!-- <a href="https://bundlesdf.github.io/" target="_blank">BundleSDF</a> -->
                    BundleSDF for vision-based
                    tracking and shape reconstruction, and PLL for physics-inspired dynamics learning. Beyond the insights
                    that led to this systems integration, our main contribution lies in how Vysics incorporates
                    these two powerful tools together such that they supervise each other and output an object dynamics
                    model, featuring geometry informed by both vision and contact.
                </p>
            </div>

            <br>

            <div class="base-row add-top-padding">
                <h1 id="abstract">Results</h1>
            </div>
            <div class="base-row">

                <div class="base-row add-top-padding">

<!--                    <h3 class="results-text">-->
<!--                        <b>Click the thumbnails below to interact with the reconstructed 3D scenes.</b>-->
<!--                    </h3>-->
                </div>

<!--                <div id="iframe-container" class="iframe-container">-->
<!--                    <model-viewer id="demo1" src="./static/3d/scene1_v2.glb" shadow-intensity="1"-->
<!--                        camera-controls touch-action="pan-y" camera-orbit="45deg 45deg 120m" ar-->
<!--                        ar-modes="webxr scene-viewer quick-look" ar-status="not-presenting"-->
<!--                        class="iframe"-->
<!--                        style="display: none;"></model-viewer>-->
<!--                    <model-viewer id="demo2" src="./static/3d/scene17.glb" shadow-intensity="1"-->
<!--                        camera-controls touch-action="pan-y" camera-orbit="45deg 45deg 120m" ar-->
<!--                        ar-modes="webxr scene-viewer quick-look" ar-status="not-presenting"-->
<!--                        class="iframe"-->
<!--                        style="display: none;"></model-viewer>-->
<!--                    <model-viewer id="demo3" src="./static/3d/scene22.glb" shadow-intensity="1"-->
<!--                        camera-controls touch-action="pan-y" camera-orbit="45deg 45deg 120m" ar-->
<!--                        ar-modes="webxr scene-viewer quick-look" ar-status="not-presenting"-->
<!--                        class="iframe"-->
<!--                        style="display: none;"></model-viewer>-->
<!--                    <model-viewer id="demo4" src="./static/3d/scene23.glb" shadow-intensity="1"-->
<!--                        camera-controls touch-action="pan-y" camera-orbit="45deg 45deg 120m" ar-->
<!--                        ar-modes="webxr scene-viewer quick-look" ar-status="not-presenting"-->
<!--                        class="iframe"-->
<!--                        style="display: none;"></model-viewer>-->
<!--                    <model-viewer id="demo5" src="./static/3d/scene36.glb" shadow-intensity="1"-->
<!--                        camera-controls touch-action="pan-y" camera-orbit="45deg 45deg 120m" ar-->
<!--                        ar-modes="webxr scene-viewer quick-look" ar-status="not-presenting"-->
<!--                        class="iframe"-->
<!--                        style="display: none;"></model-viewer>-->
<!--                    <model-viewer id="demo6" src="./static/3d/scene_20_teaser.glb" shadow-intensity="1"-->
<!--                        camera-controls touch-action="pan-y" camera-orbit="45deg 45deg 120m" ar-->
<!--                        ar-modes="webxr scene-viewer quick-look" ar-status="not-presenting"-->
<!--                        class="iframe"-->
<!--                        ></model-viewer>-->
<!--                    <model-viewer id="demo7" src="./static/3d/scene30.glb" shadow-intensity="1"-->
<!--                        camera-controls touch-action="pan-y" camera-orbit="45deg 45deg 120m" ar-->
<!--                        ar-modes="webxr scene-viewer quick-look" ar-status="not-presenting"-->
<!--                        class="iframe"-->
<!--                        style="display: none;"></model-viewer>-->
<!--                </div>-->


<!--                <div id="main-results" onclick="play_pause()">-->
<!--                    <canvas id="canvas"></canvas>-->
<!--                    <canvas id="canvas-overlay"></canvas>-->
<!--                </div>-->


<!--                <div class="base-row" style="display: inline-block">-->
<!--                    <button class="slide-arrow" id="slide-arrow-prev" onclick="slide_left()">-->
<!--                        &#8249;-->
<!--                    </button>-->

<!--                    <div class="base-row thumbnail-row" , id="thumbnails-scroll">-->
<!--                        <div class="base-col thumbnail-col">-->
<!--                            <button class="thumbnail-btn" id="thumb-5">-->
<!--                                <img src="./static/images/thumbnails/scene6.png" alt="paper" class="thumbnails" />-->
<!--                                Scene1-->
<!--                            </button>-->
<!--                        </div>-->
<!--                        <div class="base-col thumbnail-col">-->
<!--                            <button class="thumbnail-btn" id="thumb-0">-->
<!--                                <img src="./static/images/thumbnails/scene1.png" alt="paper" class="thumbnails" />-->
<!--                                Scene2-->
<!--                            </button>-->
<!--                        </div>-->
<!--                        <div class="base-col thumbnail-col">-->
<!--                            <button class="thumbnail-btn" id="thumb-1">-->
<!--                                <img src="./static/images/thumbnails/scene2.png" alt="paper" class="thumbnails" />-->
<!--                                Scene3-->
<!--                            </button>-->
<!--                        </div>-->
<!--                        <div class="base-col thumbnail-col">-->
<!--                            <button class="thumbnail-btn" id="thumb-2">-->
<!--                                <img src="./static/images/thumbnails/scene3.png" alt="paper" class="thumbnails" />-->
<!--                                Scene4-->
<!--                            </button>-->
<!--                        </div>-->
<!--                        <div class="base-col thumbnail-col">-->
<!--                            <button class="thumbnail-btn" id="thumb-3">-->
<!--                                <img src="./static/images/thumbnails/scene4.png" alt="paper" class="thumbnails" />-->
<!--                                Scene5-->
<!--                            </button>-->
<!--                        </div>-->
<!--                        <div class="base-col thumbnail-col">-->
<!--                            <button class="thumbnail-btn" id="thumb-4">-->
<!--                                <img src="./static/images/thumbnails/scene5.png" alt="paper" class="thumbnails" />-->
<!--                                Scene6-->
<!--                            </button>-->
<!--                        </div>-->
<!--                        <div class="base-col thumbnail-col">-->
<!--                            <button class="thumbnail-btn" id="thumb-6">-->
<!--                                <img src="./static/images/thumbnails/scene7.png" alt="paper" class="thumbnails" />-->
<!--                                Scene7-->
<!--                            </button>-->
<!--                        </div>-->
<!--                    </div>-->
<!--                    <button class="slide-arrow" id="slide-arrow-next" onclick="slide_right()">-->
<!--                        &#8250;-->
<!--                    </button>-->
<!--                </div>-->
<!--                </div>-->
<!--                <br>-->
<!--                <br>-->

<!--                <div class="base-row add-top-padding">-->
<!--                    <h1>Evaluating SceneComplete</h1>-->
<!--                    <p class="paragraph">-->
<!--                        We evaluate <b>SceneComplete</b> on the challenging GraspNet-1Billion dataset and observe that the parallel jaw grasps sampled using antipodal grasp sampling enable more robust predictions with fewer collisions on the ground truth scene. We also demnstrate dexterous grasps using both Shadow Hands and Allegro Hands, highlighting improved manipulation of complex objects with complete 3D reconstructions.-->
<!--                    </p>-->
<!--                </div>-->

<!--                <img id="pipeline-img" src="./static/images/qualitative.png" />-->
                <div class="results-grid">
                    <!-- Row 1 -->
                    <div class="result-row">
                        <img src="./static/images/object_photos/bakingbox.png"
                            alt="Object 1" class="result-image-no-border">
                        <video autoplay loop muted playsinline
                            class="result-video-no-border">
                            <source src="./static/images/bakingbox_3/robotocc_bakingbox_3_bsdf_00_00-t11_1_mesh.mp4"
                                type="video/mp4">
                        </video>
                    </div>

                    <!-- Row 2 -->
                    <div class="result-row">
                        <img src="./static/images/object_photos/bottle.png"
                            alt="Object 2" class="result-image-no-border">
                        <video autoplay loop muted playsinline
                            class="result-video-no-border">
                            <source src="./static/images/bottle_1/robotocc_bottle_1_bsdf_00_00-t09d_1_mesh.mp4"
                                type="video/mp4">
                        </video>
                    </div>

                    <!-- Row 3 -->
                    <div class="result-row">
                        <img src="./static/images/object_photos/egg.png"
                            alt="Object 3" class="result-image-no-border">
                        <video autoplay loop muted playsinline
                            class="result-video-no-border">
                            <source src="./static/images/egg_6/robotocc_egg_6_bsdf_00_00-t11_1_mesh.mp4"
                                type="video/mp4">
                        </video>
                    </div>

                    <!-- Row 4 -->
                    <div class="result-row">
                        <img src="./static/images/object_photos/milk.png"
                            alt="Object 4" class="result-image-no-border">
                        <video autoplay loop muted playsinline
                            class="result-video-no-border">
                            <source src="./static/images/milk_8/robotocc_milk_8_bsdf_00_00-t11_1_mesh.mp4"
                                type="video/mp4">
                        </video>
                    </div>

                    <!-- Row 5 -->
                    <div class="result-row">
                        <img src="./static/images/object_photos/oatly.png"
                            alt="Object 5" class="result-image-no-border">
                        <video autoplay loop muted playsinline
                            class="result-video-no-border">
                            <source src="./static/images/oatly_11/robotocc_oatly_11_bsdf_00_00-t11_1_mesh.mp4"
                                type="video/mp4">
                        </video>
                    </div>

                    <!-- Row 6 -->
                    <div class="result-row">
                        <img src="./static/images/object_photos/styrofoam.png"
                            alt="Object 6" class="result-image-no-border">
                        <video autoplay loop muted playsinline
                            class="result-video-no-border">
                            <source src="./static/images/styrofoam_1/robotocc_styrofoam_1_bsdf_00_00-t09d_1_mesh.mp4"
                                type="video/mp4">
                        </video>
                    </div>

                    <!-- Row 7 -->
                    <div class="result-row">
                        <img src="./static/images/object_photos/toblerone.png"
                            alt="Object 7" class="result-image-no-border">
                        <video autoplay loop muted playsinline
                            class="result-video-no-border">
                            <source src="./static/images/toblerone_11/robotocc_toblerone_11_bsdf_00_00-t11_1_mesh.mp4"
                            type="video/mp4">
                        </video>
                    </div>
                </div>
                <br>
                <br>

                <div class="base-row add-top-padding">
                    <h1>Geometry Reconstruction</h1>
                    <p class="paragraph">
                        A quantitative comparison of the geometric reconstruction
                        between Vysics and others is provided in Table I, averaged
                        per object and over all objects. [Place holder] compares example-
                        by-example with BundleSDF, the only geometric baseline
                        that, like Vysics, does not require a pretrained model on a
                        large object dataset.
                    </p>
                </div>

            <div class="base-row add-top-padding">
                <h1>Dynamic Predictions</h1>
                <p class="paragraph">
                    We further use dynamics predictions to show that the
                    geometry estimated by our method better explains the observed
                    trajectory. [Place holder] shows the average position and rotation
                    errors when predicting the entire length of the original trajectory
                    as an open-loop rollout. The trajectories range in length
                    from 3 to 18 seconds. Compared with the shape estimated
                    by the vision-only method, the geometry estimated by our
                    method enabled much smaller simulation pose errors across
                    the dataset.
                </p>
            </div>

            <div class="base-row add-top-padding">
                <h1>Data-Driven Generative Baselines</h1>
                <p class="paragraph">
                    While the objects are heavily occluded in the camera
                    view, one may wonder if 3D foundation models, which learn
                    prior knowledge of the shape of typical objects from a large
                    amount of data, may recover the occluded geometry in a
                    generative way. We tested a few single-view reconstruction
                    models, and found that they typically assume
                    an unobstructed view of the object and do not generate a
                    complete shape when given a partially occluded view. See
                    Figure 12 for an example. Foundation models are not yet a
                    hand-waving solution to shape reconstruction under occlusion,
                    but it could be valuable to incorporate a data-based prior in
                    our framework in the future.
                </p>
            </div>

                <div class="columns is-centered">

                    <!-- Relocalization (particle filter) -->
                    <div class="column">
                        <div class="content">
                            <div class="interpolation-image-wrapper-zero-shot">
                                <video poster="" id="snoopy" autoplay controls muted loop playsinline height="100%">
                                    <source src="./static/images/dexterous_grasping/dexterous1.mp4" type="video/mp4">
                                </video>
                            </div>
                        </div>
                    </div>
                    <!--/ Relocalization (particle filter) -->

                    <!-- Text queries via CLIP / GPT-4 -->
                    <div class="column">
                        <div class="content">
                            <div class="interpolation-image-wrapper-zero-shot">
                                <video poster="" id="snoopy" autoplay controls muted loop playsinline height="100%">
                                    <source src="./static/images/dexterous_grasping/dexterous2.mp4" type="video/mp4">
                                </video>
                            </div>
                        </div>
                    </div>
                </div>

                <div class="citation add-top-padding">
                    <h1 id="abstract"> Citation </h1>
                    <p> If you find this work useful, please consider citing: (bibtex) </p>
                    <pre id="codecell0">@inproceedings{bianchini2025vysics,
&nbsp;title={Vysics: Object Reconstruction Under Occlusion by Fusing Vision and Contact-Rich Physics},
&nbsp;author={Bibit Bianchini and Minghan Zhu and Mengti Sun and Bowen Jiang and Camillo J. Taylor and Michael Posa},
&nbsp;year={2025},
&nbsp;month={june},
&nbsp;arxiv={TODO},
&nbsp;booktitle={Robotics: Science and Systems (RSS)},
&nbsp;website={https://vysics.github.io/},
&nbsp;url={https://arxiv.org/abs/TODO}
}
</pre>
                </div>
            </div>
        </div>
    </div>

    		
    <p style="text-align:left;font-size:small;padding: 1%;">
        Source code for this page was taken from <a href="https://scenecomplete.github.io">SceneComplete's website</a>.
    </p>


    <script>
        const carousel = document.querySelector('.carousel-slider');
        const videoContainers = document.querySelectorAll('.video-container');
        let currentPosition = 0;

        const leftButton = document.querySelector('.carousel-button.left');
        leftButton.addEventListener('click', () => {
            if (currentPosition - 1 < 0) {
                currentPosition = videoContainers.length - 1;
            } else {
                currentPosition = currentPosition - 1;
            }

            carousel.scrollTo({ left: videoContainers[currentPosition].offsetLeft, behavior: 'smooth' });
        });

        const rightButton = document.querySelector('.carousel-button.right');
        rightButton.addEventListener('click', () => {
            if (currentPosition + 1 > videoContainers.length - 1) {
                currentPosition = 0;
            } else {
                currentPosition = currentPosition + 1;
            }
            // currentPosition = Math.min(currentPosition + 1, videoContainers.length - 1);
            carousel.scrollTo({ left: videoContainers[currentPosition].offsetLeft, behavior: 'smooth' });
        });
    </script>
    <div hidden="hidden">
        <script type="text/javascript" id="clustrmaps" src="//clustrmaps.com/map_v2.js?d=Fp3OV0D2ycF0jHT6VcMi3CdJFG_vn6lt5jKJI4zhJYQ&cl=ffffff&w=a"></script>
    </div>
</body>
</html>
