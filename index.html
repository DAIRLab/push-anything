<html>

<head>


    <meta charset="utf-8" />
    <title>Vysics</title>

    <meta content="SceneComplete is an open-world 3D scene completion system, that constructs a complete, segmented, 3D model of a scene from a single RGB-D image."
        name="description" />
    <meta content="SceneComplete is an open-world 3D scene completion system." property="og:title" />
    <meta content="SceneComplete is an open-world 3D scene completion system, that constructs a complete, segmented, 3D model of a scene from a single RGB-D image."
        name="description"
        property="og:description" />
    <meta content="https://scenecomplete.github.io/static/images/meta.png" property="og:image" />
    <meta content="SceneComplete is an open-world 3D scene completion system." property="twitter:title" />
    <meta content="SceneComplete is an open-world 3D scene completion system, that constructs a complete, segmented, 3D model of a scene from a single RGB-D image."
        name="description"
        property="twitter:description" />
    <meta content="https://scenecomplete.github.io/static/images/meta.png" property="twitter:image" />
    <meta property="og:type" content="website" />
    <meta content="summary_large_image" name="twitter:card" />
    <meta name="viewport" content="width=device-width, initial-scale=1, minimum-scale=1" />


    <link href="https://fonts.googleapis.com" rel="preconnect" />
    <link href="https://fonts.gstatic.com" rel="preconnect" crossorigin="anonymous" />
    <script src="https://ajax.googleapis.com/ajax/libs/webfont/1.6.26/webfont.js" type="text/javascript"></script>
    <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro" rel="stylesheet">
    <script
        type="text/javascript">WebFont.load({ google: { families: ["Lato:100,100italic,300,300italic,400,400italic,700,700italic,900,900italic", "Montserrat:100,100italic,200,200italic,300,300italic,400,400italic,500,500italic,600,600italic,700,700italic,800,800italic,900,900italic", "Ubuntu:300,300italic,400,400italic,500,500italic,700,700italic", "Open Sans:300,300italic,400,400italic,600,600italic,700,700italic,800,800italic", "Changa One:400,400italic", "Varela Round:400", "Bungee Shade:regular", "Roboto:300,regular,500", "Bungee Outline:regular"] } });</script>
    <!--[if lt IE 9]><script src="https://cdnjs.cloudflare.com/ajax/libs/html5shiv/3.7.3/html5shiv.min.js" type="text/javascript"></script><![endif]-->

    <script src="https://ajax.googleapis.com/ajax/libs/jquery/1/jquery.min.js"></script>
    <script src="script.js" type="text/javascript"></script>

    <link href="style.css" rel="stylesheet" type="text/css" />

    <link href="./static/images/favicon.svg" rel="shortcut icon" type="image/x-icon" />

    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/4.7.0/css/font-awesome.min.css">
    <!-- <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/bulma@0.9.4/css/bulma.min.css"> -->

<!--     <script>
        // Check if the user is on a mobile device or the window width is 800px or less.
        if (window.innerWidth <= 800 || /Android|webOS|iPhone|iPad|iPod|BlackBerry|IEMobile|Opera Mini/i.test(navigator.userAgent)) {
            window.location.href = "mobile.html";  // Redirect to mobile version
        }
    </script> -->
    <script type="module" src="https://ajax.googleapis.com/ajax/libs/model-viewer/3.4.0/model-viewer.min.js"></script>
</head>

<body>
    <div class="section">
        <div class="container">
            <br>
            <h1 class="title">Vysics</h1>
            <h1 class="subheader">Object Reconstruction Under Occlusion by
                Fusing Vision and Contact-Rich Physics</h1>

            <div class="publication-authors" style="font-size:1.25rem;">
                <span class="author-block">
                    <a href="https://scholar.google.com/citations?user=lVj0WaQAAAAJ&hl=en" target="_blank">Bibit Bianchini
                    </a><sup>*1</sup>,</span>
                <span class="author-block">
                    <span class="author-block">
                        <a href="https://scholar.google.com/citations?user=70CbUXwAAAAJ&hl=en" target="_blank">Minghan Zhu
                        </a><sup>*1</sup>,</span>
                    <span class="author-block">
                        <a href="https://scholar.google.com/citations?user=hXb_RHoAAAAJ&hl=en" target="_blank">Mengti Sun</a><sup>2</sup>,
                    </span>
                    <span class="author-block">
                        <a href="https://scholar.google.com/citations?user=_6AHV9QAAAAJ&hl=en" target="_blank">Bowen Jiang</a><sup>1</sup>,
                    </span>
                    <span class="author-block">
                        <a href="https://www.cis.upenn.edu/~cjtaylor/" target="_blank">Camillo J. Taylor
                        </a><sup>1</sup>
                    </span>
                    <span class="author-block">
                        <a href="https://www.grasp.upenn.edu/people/michael-posa/" target="_blank">Michael Posa
                        </a><sup>1</sup>
                    </span>
            </div>
            <br>
            <div class="publication-authors" style="font-size:1.25rem;">
                <span class="author-block" style="display:block"><sup>*</sup>The first two authors contributed equally to this work.</span>

                <span class="author-block" style="display:block"><sup>1</sup>General Robotics, Automation, Sensing and Perception (GRASP) Laboratory, University of Pennsylvania </span>

                <span class="author-block" style="display:block"><sup>2</sup>Amazon Robotics</span>
            </div>
            <br>

            <div class="link-labels base-row">
                <!-- TODO: Update arxiv link -->
                <div class="base-col icon-col">
                    <!-- <a href="https://arxiv.org/abs/2410.23643" target="_blank"
                        class="link-block"> -->
                    <img src="https://uploads-ssl.webflow.com/51e0d73d83d06baa7a00000f/5cab99df4998decfbf9e218e_paper-01.png"
                        alt="paper"
                        sizes="(max-width: 479px) 12vw, (max-width: 767px) 7vw, (max-width: 991px) 41.8515625px, 56.6953125px"
                        srcset="https://uploads-ssl.webflow.com/51e0d73d83d06baa7a00000f/5cab99df4998decfbf9e218e_paper-01-p-500.png 500w, https://uploads-ssl.webflow.com/51e0d73d83d06baa7a00000f/5cab99df4998decfbf9e218e_paper-01.png 672w"
                        class="icon-img" />
                    </a>
                </div>
                <!-- TODO: Update code link -->
                <div class="base-col icon-col">
                    <!-- <a href="https://github.com/ebianchi/bundlenets/tree/structural-changes"
                        target="_blank" > -->
                    <img src="https://uploads-ssl.webflow.com/51e0d73d83d06baa7a00000f/5cae3b53b42ebb3dd4175a82_68747470733a2f2f7777772e69636f6e66696e6465722e636f6d2f646174612f69636f6e732f6f637469636f6e732f313032342f6d61726b2d6769746875622d3235362e706e67.png"
                        alt="code" class="icon-img github-img-icon" />
                    </a>
                </div>
                <!-- TODO: Update dataset link -->
                <div class="base-col icon-col">
                    <!-- <a href="https://github.com/ebianchi/bundlenets/tree/structural-changes"
                        target="_blank" > -->
                    <img src="./static/logos/dataset_icon.png"
                        alt="code" class="icon-img github-img-icon" />
                    </a>
                </div>
                <!-- TODO: Update Youtube link -->
                <div class="column-2 base-col icon-col">
                    <!-- <a href="https://youtu.be/Tuzhn4HWiL0" target="_blank"
                        class="link-block"> -->
                    <img src="./static/logos/youtube_1.svg"
                        alt="video" class="icon-img data-img-icon" />
                    </a>
                </div>
            </div>
            <div class="link-labels base-row">
                <div class="base-col icon-col">
                    <strong class="link-labels-text">Paper<br>(coming soon)</strong>
                </div>
                <div class="base-col icon-col">
                    <strong class="link-labels-text">Code<br>(coming soon)</strong>
                </div>
                <div class="base-col icon-col">
                    <strong class="link-labels-text">Dataset<br>(coming soon)</strong>
                </div>
                <div class="base-col icon-col">
                    <strong class="link-labels-text">Video<br>(coming soon)</strong>
                </div>
            </div>

            <br>

            <!-- Main video -->
            <div class="base-row add-top-padding">
                <video id="main-video" autobuffer muted autoplay loop>
                    <source id="mp4"
                        src="./static/images/vysics_intro_small.mp4"
                        type="video/mp4">
                </video>
            </div>

            <h1 class="tldr">
                <b>TL;DR</b>:
                <u>Vysics</u> is a vision-and-physics framework for a robot to build an expressive geometry and dynamics
                model of a single rigid body, using a seconds-long RGBD video and the robot’s proprioception.
            </h1>

            <br>

            <!-- Abstract -->
            <div class="base-row add-top-padding">
                <h1 id="abstract">Abstract</h1>
                <p class="paragraph">
                    We introduce <b>Vysics</b>, a vision-and-physics framework for a robot to build an expressive geometry and dynamics
                    model of a single rigid body, using a seconds-long RGBD video
                    and the robot's proprioception. While the computer vision community has built powerful visual 3D perception algorithms, cluttered environments with heavy occlusions can limit the visibility
                    of objects of interest. However, observed motion of partially occluded objects can imply physical interactions took place, such as
                    contact with a robot or the environment. These inferred contacts
                    can supplement the visible geometry with “physible geometry,”
                    which best explains the observed object motion through physics.
                    Vysics uses a vision-based tracking and reconstruction method,
                    BundleSDF, to estimate the trajectory and the visible geometry
                    from an RGBD video, and an odometry-based model learning
                    method, Physics Learning Library (PLL), to infer the “physible”
                    geometry from the trajectory through implicit contact dynamics
                    optimization. The visible and “physible” geometries jointly factor
                    into optimizing a signed distance function (SDF) to represent the
                    object shape. Vysics does not require pretraining, nor tactile
                    or force sensors. Compared with vision-only methods, Vysics
                    yields object models with higher geometric accuracy and better
                    dynamics prediction in experiments where the object interacts
                    with the robot and the environment under heavy occlusion.
                </p>
            </div>

            <br>

            <!-- Architecture -->
            <div class="base-row add-top-padding">
                <h1 id="abstract">Architecture</h1>
                <video id="arch-video" autoplay muted loop playsinline height="100%">
                    <source src="./static/images/vysics_architecture_animation.mp4" type="video/mp4">
                </video>
                <p class="paragraph">
                    The figure illustrates the overall design of <b>Vysics</b> from input RGBD videos and
                    robot states (left) to URDF output (right). Its core components are
                    <!-- TODO:  link to BundleSDF? -->
                    <!-- <a href="https://bundlesdf.github.io/" target="_blank">BundleSDF</a> -->
                    BundleSDF for vision-based
                    tracking and shape reconstruction, and PLL for physics-inspired dynamics learning. Beyond the insights
                    that led to this systems integration, our main contribution lies in how Vysics incorporates
                    these two powerful tools together such that they supervise each other and output an object dynamics
                    model, featuring geometry informed by both vision and contact.
                </p>
            </div>

            <br>

            <!-- Geometry results -->
            <div class="base-row add-top-padding">
                <h1 id="abstract">Geometry Results</h1>
                <nav>
                    <div class="carousel-slider-wrapper">
                        <div class = "carousel-slider">

                            <div class="video-container">
                                <video  loop playsinline muted autoplay>
                                    <source autoplay
                                        src="./static/images/carousel_geometry/geometry_milk_3_small.mp4"
                                        type="video/mp4">
                                </video>
                            </div>

                            <div class="video-container">
                                <video  loop playsinline muted autoplay>
                                    <source autoplay
                                        src="./static/images/carousel_geometry/geometry_oatly_11_small.mp4"
                                        type="video/mp4">
                                </video>
                            </div>

                            <div class="video-container">
                                <video  loop playsinline muted autoplay>
                                    <source autoplay
                                        src="./static/images/carousel_geometry/geometry_styrofoam_1_small.mp4"
                                        type="video/mp4">
                                </video>
                            </div>

                            <div class="video-container">
                                <video  loop playsinline muted autoplay>
                                    <source autoplay
                                        src="./static/images/carousel_geometry/geometry_egg_6_small.mp4"
                                        type="video/mp4">
                                </video>
                            </div>

                            <div class="video-container">
                                <video  loop playsinline muted autoplay>
                                    <source autoplay
                                        src="./static/images/carousel_geometry/geometry_bottle_1_small.mp4"
                                        type="video/mp4">
                                </video>
                            </div>

                            <div class="video-container">
                                <video  loop playsinline muted autoplay>
                                    <source autoplay
                                        src="./static/images/carousel_geometry/geometry_toblerone_11_small.mp4"
                                        type="video/mp4">
                                </video>
                            </div>

                            <div class="video-container">
                                <video  loop playsinline muted autoplay>
                                    <source autoplay
                                        src="./static/images/carousel_geometry/geometry_bakingbox_3_small.mp4"
                                        type="video/mp4">
                                </video>
                            </div>

                            <div class="video-container">
                                <video  loop playsinline muted autoplay>
                                    <source autoplay
                                        src="./static/images/carousel_geometry/geometry_milk_8_small.mp4"
                                        type="video/mp4">
                                </video>
                            </div>

                            <div class="video-container">
                                <video  loop playsinline muted autoplay>
                                    <source autoplay
                                        src="./static/images/carousel_geometry/geometry_oatly_6_small.mp4"
                                        type="video/mp4">
                                </video>
                            </div>
                        </div>
                        <button class="carousel-button left" type="button">&lt;</button>
                        <button class="carousel-button right" type="button">&gt;</button>
                    </div>
                </nav>

                <p class="paragraph">
                    Vysics trains on a seconds-long RGBD video and the robot's
                    proprioception.  Each example above shows the input video,
                    a photo of the object, and the reconstructed geometry from
                    Vysics (which fuses vision and physics) and BundleSDF (which
                    uses vision only).  The mesh coloring indicates the distance
                    between a local region of the learned shape to the nearest
                    surface on the ground truth geometry:  red is > 3cm, and
                    dark blue is < 5mm.
                </p>

                <div class="columns is-centered">
                    <div class="column">
                        <img id="geometry-img"
                            src="./static/images/geometry_chamfer.png" />
                    </div>
                    <div class="column">
                        <img id="geometry-img"
                            src="./static/images/geometry_iou.png" />
                    </div>
                </div>

                <p class="paragraph">
                    Due to severe occlusion in the RGBD videos, vision-based
                    reconstruction yields significant errors.  For every example
                    in our dataset, our method improves the reconstructed
                    geometry by recovering the occluded geometry through
                    physics-based reasoning over the observed trajectories.
                    The plots above show substantial and consistent improvements
                    in terms of both the surface-based metric (chamfer distance)
                    and the volume-based metric (IoU).
                </p>
            </div>

            <br>

            <!-- Geometry reconstruction -->
            <div class="base-row add-top-padding">
                <h1>Geometry Reconstruction</h1>
                <p class="paragraph">
                    A quantitative comparison of the geometric reconstruction
                    between Vysics and others is provided in Table I, averaged
                    per object and over all objects. [Place holder] compares example-
                    by-example with BundleSDF, the only geometric baseline
                    that, like Vysics, does not require a pretrained model on a
                    large object dataset.
                </p>
            </div>

            <br>

            <!-- Dynamics predictions -->
            <div class="base-row add-top-padding">
                <h1 id="abstract">Dynamics Predictions</h1>
                <nav>
                    <div class="carousel-slider-wrapper">
                        <div class = "carousel-slider">

                            <div class="video-container">
                                <video  loop playsinline muted autoplay>
                                    <source autoplay
                                        src="./static/images/carousel_dynamics/dynamics_milk_3_small.mp4"
                                        type="video/mp4">
                                </video>
                            </div>

                            <div class="video-container">
                                <video  loop playsinline muted autoplay>
                                    <source autoplay
                                        src="./static/images/carousel_dynamics/dynamics_oatly_11_small.mp4"
                                        type="video/mp4">
                                </video>
                            </div>

                            <div class="video-container">
                                <video  loop playsinline muted autoplay>
                                    <source autoplay
                                        src="./static/images/carousel_dynamics/dynamics_styrofoam_1_small.mp4"
                                        type="video/mp4">
                                </video>
                            </div>

                            <div class="video-container">
                                <video  loop playsinline muted autoplay>
                                    <source autoplay
                                        src="./static/images/carousel_dynamics/dynamics_egg_6_small.mp4"
                                        type="video/mp4">
                                </video>
                            </div>

                            <div class="video-container">
                                <video  loop playsinline muted autoplay>
                                    <source autoplay
                                        src="./static/images/carousel_dynamics/dynamics_bottle_1_small.mp4"
                                        type="video/mp4">
                                </video>
                            </div>

                            <div class="video-container">
                                <video  loop playsinline muted autoplay>
                                    <source autoplay
                                        src="./static/images/carousel_dynamics/dynamics_toblerone_11_small.mp4"
                                        type="video/mp4">
                                </video>
                            </div>

                            <div class="video-container">
                                <video  loop playsinline muted autoplay>
                                    <source autoplay
                                        src="./static/images/carousel_dynamics/dynamics_bakingbox_3_small.mp4"
                                        type="video/mp4">
                                </video>
                            </div>

                            <div class="video-container">
                                <video  loop playsinline muted autoplay>
                                    <source autoplay
                                        src="./static/images/carousel_dynamics/dynamics_milk_8_small.mp4"
                                        type="video/mp4">
                                </video>
                            </div>

                            <div class="video-container">
                                <video  loop playsinline muted autoplay>
                                    <source autoplay
                                        src="./static/images/carousel_dynamics/dynamics_oatly_6_small.mp4"
                                        type="video/mp4">
                                </video>
                            </div>
                        </div>
                        <button class="carousel-button left" type="button">&lt;</button>
                        <button class="carousel-button right" type="button">&gt;</button>
                    </div>
                </nav>

                <p class="paragraph">
                    Because Vysics uses physics reasoning to inform the
                    recovered geometry, it enables accurate dynamics predictions
                    when predicting the original trajectory.  To do this, we
                    simulate the recorded robot commands in a simulator with the
                    learned object model, then we compare the resulting object
                    trajectory to the BundleSDF trajectory estimate.  The videos
                    above show the simulation results of Vysics models compared
                    to vision-only geometry (from BundleSDF), physics-only
                    geometry (from PLL), and the ground truth geometry (from a
                    3D scanner).
                </p>

                <img id="dynamics-img" src="./static/images/dynamics_cdf.png" />

                <p class="paragraph">
                    The plot above shows the average position and rotation
                    errors when predicting the entire length of the original
                    trajectory as an open-loop rollout, averaged for every
                    example and result in the dataset. The trajectories range in
                    length from 3 to 18 seconds.  We point out that even the
                    ground truth geometry baseline is imperfect, despite using
                    essentially perfect geometry, due to inaccurate modeling
                    assumptions such as object rigidity and the divergent nature
                    of the dynamics in many of our robot interactions.  Vysics
                    and PLL perform closely to this baseline, though Vysics is
                    moderately worse in orientation divergence.  While most of
                    the dynamics performance by PLL is retained in Vysics, it is
                    unsurprising to see a slight performance drop, given PLL
                    optimizes only for physics accuracy while Vysics balances
                    with visual objectives.  The vision-only baseline is the
                    least performant in both position and orientation rollout
                    accuracy.
                </p>
            </div>

            <br>

            <!-- Data-driven generative baselines -->
            <div class="base-row add-top-padding">
                <h1>Data-Driven Generative Baselines</h1>
                <p class="paragraph">
                    While the objects are heavily occluded in the camera
                    view, one may wonder if 3D foundation models, which learn
                    prior knowledge of the shape of typical objects from a large
                    amount of data, may recover the occluded geometry in a
                    generative way. We tested a few single-view reconstruction
                    models, and found that they typically assume
                    an unobstructed view of the object and do not generate a
                    complete shape when given a partially occluded view. See
                    Figure 12 for an example. Foundation models are not yet a
                    hand-waving solution to shape reconstruction under occlusion,
                    but it could be valuable to incorporate a data-based prior in
                    our framework in the future.
                </p>

                <div class="columns is-centered">

                    <!-- Relocalization (particle filter) -->
                    <div class="column">
                        <div class="content">
                            <div class="interpolation-image-wrapper-zero-shot">
                                <video poster="" id="snoopy" autoplay controls muted loop playsinline height="100%">
                                    <source src="./static/images/dexterous_grasping/dexterous1.mp4" type="video/mp4">
                                </video>
                            </div>
                        </div>
                    </div>
                    <!--/ Relocalization (particle filter) -->

                    <!-- Text queries via CLIP / GPT-4 -->
                    <div class="column">
                        <div class="content">
                            <div class="interpolation-image-wrapper-zero-shot">
                                <video poster="" id="snoopy" autoplay controls muted loop playsinline height="100%">
                                    <source src="./static/images/dexterous_grasping/dexterous2.mp4" type="video/mp4">
                                </video>
                            </div>
                        </div>
                    </div>
                </div>
            </div>

            <br>

            <!-- Citation -->
            <div class="citation add-top-padding">
                <h1 id="abstract"> Citation </h1>
                <p> If you find this work useful, please consider citing: (bibtex) </p>
                <pre id="codecell0">@inproceedings{bianchini2025vysics,
&nbsp;title={Vysics: Object Reconstruction Under Occlusion by Fusing Vision and Contact-Rich Physics},
&nbsp;author={Bibit Bianchini and Minghan Zhu and Mengti Sun and Bowen Jiang and Camillo J. Taylor and Michael Posa},
&nbsp;year={2025},
&nbsp;month={june}, <!-- &nbsp;arxiv={TODO}, -->
&nbsp;booktitle={Robotics: Science and Systems (RSS)},
&nbsp;website={https://vysics.github.io/}, <!-- &nbsp;url={https://arxiv.org/abs/TODO} -->
}
</pre>
            </div>

            </div>
        </div>
    </div>

    		
    <p style="text-align:left;font-size:small;padding: 1%;">
        Source code for this page was taken from
        <a href="https://scenecomplete.github.io">SceneComplete's website</a>.
    </p>


    <script>
        const carousel = document.querySelector('.carousel-slider');
        const videoContainers = document.querySelectorAll('.video-container');
        let currentPosition = 0;

        const leftButton = document.querySelector('.carousel-button.left');
        leftButton.addEventListener('click', () => {
            if (currentPosition - 1 < 0) {
                currentPosition = videoContainers.length - 1;
            } else {
                currentPosition = currentPosition - 1;
            }

            carousel.scrollTo({
                left: videoContainers[currentPosition].offsetLeft,
                behavior: 'smooth'
            });
        });

        const rightButton = document.querySelector('.carousel-button.right');
        rightButton.addEventListener('click', () => {
            if (currentPosition + 1 > videoContainers.length - 1) {
                currentPosition = 0;
            } else {
                currentPosition = currentPosition + 1;
            }
            carousel.scrollTo({
                left: videoContainers[currentPosition].offsetLeft,
                behavior: 'smooth'
            });
        });
    </script>
    <div hidden="hidden">
        <script type="text/javascript" id="clustrmaps"
            src="//clustrmaps.com/map_v2.js?d=Fp3OV0D2ycF0jHT6VcMi3CdJFG_vn6lt5jKJI4zhJYQ&cl=ffffff&w=a">
        </script>
    </div>
</body>
</html>
